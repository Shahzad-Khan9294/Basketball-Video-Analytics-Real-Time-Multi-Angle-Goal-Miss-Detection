{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ut-pMsFCeyP8",
        "outputId": "46dc5758-4e43-4f1d-e5b5-b88d3174e6d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqNWpNubb8lr"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Define class labels\n",
        "CLASSES_BASKET = ['ball', 'net']\n",
        "CLASSES_GOAL = ['goal']\n",
        "\n",
        "def initialize_interpreter(model_path):\n",
        "    '''Initialize TensorFlow Lite interpreter\n",
        "    '''\n",
        "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "    interpreter.allocate_tensors()\n",
        "    return interpreter\n",
        "\n",
        "def preprocess_frame(frame, input_size, is_grayscale):\n",
        "    ''' Preprocess the input frame to feed to the TFLite model.\n",
        "        Handles both RGB and grayscale inputs.\n",
        "    '''\n",
        "    img = tf.convert_to_tensor(frame)\n",
        "\n",
        "    # if is_grayscale:\n",
        "    #     # Convert to grayscale using TensorFlow\n",
        "    #     img = tf.image.rgb_to_grayscale(img)\n",
        "\n",
        "    resized_img = tf.image.resize(img, input_size)\n",
        "    resized_img = resized_img[tf.newaxis, :]  # Add batch dimension\n",
        "    resized_img = tf.cast(resized_img, dtype=tf.uint8)\n",
        "    return resized_img\n",
        "\n",
        "def detect_objects(interpreter, frame, threshold):\n",
        "    ''' Returns a list of detection results for the frame\n",
        "    '''\n",
        "    signature_fn = interpreter.get_signature_runner()\n",
        "    output = signature_fn(images=frame)\n",
        "\n",
        "    # Extract outputs as arrays\n",
        "    count = int(output['output_0'][0])\n",
        "    scores = output['output_1'][0]\n",
        "    classes = output['output_2'][0]\n",
        "    boxes = output['output_3'][0]\n",
        "\n",
        "    # Filter valid results using NumPy\n",
        "    valid_indices = scores >= threshold\n",
        "    valid_scores = scores[valid_indices]\n",
        "    valid_classes = classes[valid_indices]\n",
        "    valid_boxes = boxes[valid_indices]\n",
        "\n",
        "    # Construct results\n",
        "    results = [\n",
        "        {\n",
        "            'bounding_box': valid_boxes[i],\n",
        "            'class_id': int(valid_classes[i]),\n",
        "            'score': valid_scores[i]\n",
        "        }\n",
        "        for i in range(len(valid_scores))\n",
        "    ]\n",
        "    return results\n",
        "\n",
        "def get_basket_roi(input_video_path, interpreter, threshold, num_frames):\n",
        "    ''' Detect the basket's ROI from the initial frames of the video\n",
        "    '''\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"Error opening video file: {input_video_path}\")\n",
        "\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    _, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']\n",
        "\n",
        "    roi = None\n",
        "    for fr in range(num_frames):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        preprocessed_frame = preprocess_frame(frame, (input_height, input_width))\n",
        "        detections = detect_objects(interpreter, preprocessed_frame, threshold=threshold)\n",
        "\n",
        "        for obj in detections:\n",
        "            if CLASSES_BASKET[int(obj['class_id'])] == 'net':\n",
        "                ymin, xmin, ymax, xmax = obj['bounding_box']\n",
        "                xmin = int(xmin * frame_width)\n",
        "                xmax = int(xmax * frame_width)\n",
        "                ymin = int(ymin * frame_height)\n",
        "                ymax = int(ymax * frame_height)\n",
        "\n",
        "                width = xmax - xmin\n",
        "                height = ymax - ymin\n",
        "\n",
        "                # Expand the bounding box\n",
        "                xmin = max(0, int(xmin - 0.8 * width))  # Increase leftward expansion by 0.8 times\n",
        "                xmax = min(frame_width, int(xmax + 0.8 * width))  # Increase rightward expansion by 0.8 times\n",
        "                ymin = max(0, int(ymin - 1.8 * height))  # Increase upward expansion 1.8 times\n",
        "                ymax = min(frame_height, ymax)  # Keep the bottom fixed\n",
        "\n",
        "                roi = (xmin, ymin, xmax, ymax)\n",
        "                break\n",
        "        if roi:\n",
        "            print(f\"Basket (net) detected in the frame {fr+1}\")\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    if not roi:\n",
        "        raise ValueError(\"Basket (net) not detected in the initial frames.\")\n",
        "    return roi\n",
        "\n",
        "\n",
        "def crop_and_save_video(input_video_path, output_video_path, roi):\n",
        "    ''' Crop frames from the video based on the ROI and save as a new video\n",
        "    '''\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"Error opening video file: {input_video_path}\")\n",
        "\n",
        "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
        "    xmin, ymin, xmax, ymax = roi\n",
        "    cropped_width = xmax - xmin\n",
        "    cropped_height = ymax - ymin\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for saving video\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (cropped_width, cropped_height))\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        cropped_frame = frame[ymin:ymax, xmin:xmax]\n",
        "        out.write(cropped_frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Cropped video saved to {output_video_path}\")\n",
        "\n",
        "def process_goal_detection(input_video_path, output_video_path, model_path, threshold):\n",
        "    ''' Process the cropped video to detect goals\n",
        "    '''\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"Error opening video file: {input_video_path}\")\n",
        "\n",
        "    print(\"Processing goal detection...\")\n",
        "\n",
        "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    interpreter = initialize_interpreter(model_path)\n",
        "    input_details = interpreter.get_input_details()\n",
        "    input_shape = input_details[0]['shape'][1:3]\n",
        "\n",
        "    # Create VideoWriter object to save the output video with goal detection\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for saving video\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (frame_width, frame_height))\n",
        "\n",
        "    frame_count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Preprocess frame\n",
        "        preprocessed_frame = preprocess_frame(frame, input_shape)\n",
        "\n",
        "        # Inference on the preprocessed frame\n",
        "        detections = detect_objects(interpreter, preprocessed_frame, threshold)\n",
        "\n",
        "        # Annotate the frame\n",
        "        annotated_frame = frame.copy()  # Create a copy of the frame for annotation\n",
        "        for obj in detections:\n",
        "            box = obj['bounding_box']\n",
        "            x_min = int(box[1] * frame_width)\n",
        "            y_min = int(box[0] * frame_height)\n",
        "            x_max = int(box[3] * frame_width)\n",
        "            y_max = int(box[2] * frame_height)\n",
        "\n",
        "            # Draw bounding box and label\n",
        "            cv2.rectangle(annotated_frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
        "            label = f\"Score: {obj['score']:.2f}\"\n",
        "            cv2.putText(annotated_frame, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "        out.write(annotated_frame)  # Write annotated frame to output video\n",
        "        frame_count += 1\n",
        "        if frame_count % 500 == 0:\n",
        "          print(f\"Processed frame {frame_count}\")\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Output video with frames {frame_count} saved to {output_video_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "cwd = os.getcwd()\n",
        "MODEL_PATH = '/content/drive/MyDrive/Ballogy/Models'\n",
        "BASKET_MODEL_NAME = 'ballogy_1_eff0.tflite'\n",
        "GOAL_MODEL_NAME = 'mixed_model_3.tflite'\n",
        "DETECTION_THRESHOLD_BASKET = 0.2   # Basket detection threshold\n",
        "DETECTION_THRESHOLD_GOAL = 0.70    # Goal detection threshold\n",
        "\n",
        "# Input and output video paths\n",
        "INPUT_VIDEO_PATH = \"/content/drive/MyDrive/Ballogy_Videos/right/right_v1.mp4\"\n",
        "CROPPED_VIDEO_PATH = \"/content/drive/MyDrive/Ballogy/output/mm3_0.70/right_v1/cropped.mp4\"\n",
        "OUTPUT_VIDEO_PATH = \"/content/drive/MyDrive/Ballogy/output/mm3_0.70/right_v1/goal_detection.mp4\"\n",
        "\n",
        "# Basket detection model path\n",
        "basket_model_path = f'{MODEL_PATH}/{BASKET_MODEL_NAME}'\n",
        "basket_interpreter = initialize_interpreter(basket_model_path)\n",
        "\n",
        "# Step 1: Detect basket and crop video\n",
        "roi = get_basket_roi(INPUT_VIDEO_PATH, basket_interpreter, threshold=DETECTION_THRESHOLD_BASKET, num_frames=300)\n",
        "crop_and_save_video(INPUT_VIDEO_PATH, CROPPED_VIDEO_PATH, roi)\n",
        "\n",
        "# Goal detection model path\n",
        "goal_model_path = f'{MODEL_PATH}/{GOAL_MODEL_NAME}'\n",
        "\n",
        "# Step 2: Detect goals in cropped video\n",
        "process_goal_detection(CROPPED_VIDEO_PATH, OUTPUT_VIDEO_PATH, goal_model_path, threshold=DETECTION_THRESHOLD_GOAL)"
      ],
      "metadata": {
        "id": "wwFk74s-cWZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5de8bc3e-f2ff-4491-f5b8-93f4426083d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Basket (net) detected in the frame 142\n",
            "Cropped video saved to /content/drive/MyDrive/Ballogy/output/mm3_0.70/right_v1/cropped.mp4\n",
            "Processing goal detection...\n",
            "Processed frame 500\n",
            "Processed frame 1000\n",
            "Processed frame 1500\n",
            "Processed frame 2000\n",
            "Processed frame 2500\n",
            "Processed frame 3000\n",
            "Processed frame 3500\n",
            "Processed frame 4000\n",
            "Processed frame 4500\n",
            "Processed frame 5000\n",
            "Processed frame 5500\n",
            "Processed frame 6000\n",
            "Processed frame 6500\n",
            "Output video with frames 6784 saved to /content/drive/MyDrive/Ballogy/output/mm3_0.70/right_v1/goal_detection.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3OoJFOPlcr62"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}